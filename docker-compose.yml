version: "3.4"

networks:
  scrapy:

services:
  splash:
    image: scrapinghub/splash:master
    command: --maxrss 4000
    restart: always
    ports:
      - "127.0.0.1:8050:8050"
    networks:
      - scrapy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8050/_ping"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 40s
  headless_chrome:
    image: ghcr.io/browserless/chrome@sha256:f27f9fa0d9c2344180c0fc5af7c6ea4a1df6f2a7a3efc555de876dbea6ded7a1
    restart: always
    environment:
      - TIMEOUT=60000
    ports:
      - "127.0.0.1:3000:3000"
    networks:
      - scrapy
  scrapy:
    # extra_hosts is only required if your need to access an edu-sharing instance on the host that runs docker
    # host.docker.internal points to the ip address of the host docker network interface
    extra_hosts:
      - "host.docker.internal:host-gateway"
    image: openeduhub/oeh-search-etl:develop
    build:
      context: .
      network: host
    networks:
      - scrapy
    environment:
      - "PYPPETEER_WS_ENDPOINT=ws://headless_chrome:3000"
      - "PLAYWRIGHT_WS_ENDPOINT=ws://headless_chrome:3000"
      - "SPLASH_URL=http://splash:8050"
      - "CRAWLER=${CRAWLER}"
      # optional keyword args, e.g. cleanrun=true
      - "ARGS=${ARGS}"
      - "DRY_RUN=False"
      - "MODE=json"
      - "LOG_LEVEL=${LOG_LEVEL:-INFO}"
      - "EDU_SHARING_BASE_URL=${EDU_SHARING_BASE_URL}"
      - "EDU_SHARING_USERNAME=${EDU_SHARING_USERNAME}"
      - "EDU_SHARING_PASSWORD=${EDU_SHARING_PASSWORD}"
      - "API_MODE=${API_MODE}"
      - "Z_API_KEY=${Z_API_KEY}"
    ports:
      - "0.0.0.0:80:80"
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://0.0.0.0:80/_ping" ]
  scrapyd:
    image: openeduhub/oeh-search-etl:develop
    depends_on:
      - scrapy
      - headless_chrome
      - splash
    expose:
      - "6800"
    ports:
      - "0.0.0.0:6801:6800"
    networks:
      - scrapy
    environment:
      - "PYPPETEER_WS_ENDPOINT=ws://headless_chrome:3000"
      - "PLAYWRIGHT_WS_ENDPOINT=ws://headless_chrome:3000"
      - "SPLASH_URL=http://splash:8050"
      - "CRAWLER=${CRAWLER}"
      # optional keyword args, e.g. cleanrun=true
      - "ARGS=${ARGS}"
      - "DRY_RUN=False"
      - "MODE=json"
      - "LOG_LEVEL=${LOG_LEVEL:-INFO}"
      - "EDU_SHARING_BASE_URL=${EDU_SHARING_BASE_URL}"
      - "EDU_SHARING_USERNAME=${EDU_SHARING_USERNAME}"
      - "EDU_SHARING_PASSWORD=${EDU_SHARING_PASSWORD}"
      - "API_MODE=${API_MODE}"
      - "Z_API_KEY=${Z_API_KEY}"
    restart: "no"
    entrypoint: [ "bash", "-c", "scrapyd"]
    healthcheck:
      test: [ "CMD", "./venv_scrapyd/bin/scrapyd-deploy", "default" ]