# --- LOGGING-specific settings:
# Add a url for your log file. If not set, stdoutput will be used
#LOG_FILE = "/var/log/scrapy.log"
# Set the level for logs here. Supported values: "DEBUG", "INFO", "WARNING", "ERROR"
LOG_LEVEL="WARNING"

# --- Crawling-modes: control where crawled items should be stored/exported.
# Available modes: 'edu-sharing', 'csv', 'json' or 'None'
MODE="csv"
# ------ CSV Export settings (Only used if MODE == "csv"!):
# csv rows to export from dataset (comma seperated! field-names according to items.py!)
CSV_ROWS="lom.general.title,lom.general.description,lom.general.keyword,lom.technical.location,valuespaces.discipline,valuespaces.learningResourceType"

# --- 'Splash'-Integration settings for the local container,
# for more information, see https://splash.readthedocs.io/en/stable/
DISABLE_SPLASH=False
SPLASH_URL="http://localhost:8050"

# --- headless-browser settings for the local container:
# PYPPETEER Integration settings, as needed for the local container (as used in kmap_spider.py)
# for more information, see: https://github.com/pyppeteer/pyppeteer
PYPPETEER_WS_ENDPOINT="ws://localhost:3000"
# Playwright Integration, as needed for the local container (https://hub.docker.com/r/browserless/chrome#playwright)
PLAYWRIGHT_WS_ENDPOINT="ws://localhost:3000"

# --- Edu-Sharing instance that the crawlers should upload to
EDU_SHARING_BASE_URL="http://localhost:8080/edu-sharing/"
EDU_SHARING_USERNAME="admin"
EDU_SHARING_PASSWORD="admin"

# Configure if permissions of edu-sharing nodes are handled by the crawler (default true)
# You may want to set this to false if you don't want to apply permissions from crawlers or have a custom implementation in the repository
# EDU_SHARING_PERMISSION_CONTROL=true
# Metadataset to be used for generated nodes. You may use "default" to use the default mds of the repository
# EDU_SHARING_METADATASET=mds_oeh

# If set to true, don't upload to (above mentioned) Edu-Sharing instance
DRY_RUN=True

# --- edu-sharing Source Template Settings
# Retrieve (whitelisted) metadata properties from an edu-sharing "source template" ("Quellen-Datensatz"-Template) and
# attach those metadata properties to each item. (This feature is DISABLED by default. Possible values: True / False)
# Before enabling this feature, make sure that:
# - a "Quellen-Datensatz" exists for the <spider_name> within the specified edu-sharing repository
#   (see: EDU_SHARING_BASE_URL setting above!)
# - AND contains the "ccm:oeh_crawler_data_inherit" property !
EDU_SHARING_SOURCE_TEMPLATE_ENABLED=False
# Define a separate "source template"-repository for testing/debugging. This setting is useful if
# the "Quellen-Datensatz" does not exist on the same edu-sharing repository as defined in EDU_SHARING_BASE_URL
# (e.g., when the "Quellen-Datensatz"-Template is only available on Staging, but you want save the items on pre-Staging)
#EDU_SHARING_SOURCE_TEMPLATE_BASE_URL="http://localhost:8080/edu-sharing/"

# --- OERSI-specific settings (oersi_spider):
# Only crawl a specific metadata provider from OERSI (separate multiple providers by semicolon!):
OERSI_METADATA_PROVIDER="KI-Campus;iMoox"
# Continue / complete a previously aborted crawl process by skipping updates of already known items.
# CONTINUE_CRAWL=False
# EDU_SHARING_PRECHECK_SAVED_SEARCH_ID="<insert_your_saved_search_node_id_here>"
# Setting CONTINUE_CRAWL to True will skip all updates of previously crawled items and ONLY crawl new ones!
# ONLY use this mode if you wish to debug/complement/complete huge crawl processes which haven't completed on their own!

# --- Scrapy Pipeline settings:
# You can add one or more custom pipelines here to trigger.
# The syntax is: pipeline.package.id:PRIORITY[,pipeline.package.id:PRIORITY,...]
# Use this if you e.g. want to do custom property mapping for any crawler before storing the data
# CUSTOM_PIPELINES = "converter.pipelines.ExampleLoggingPipeline:100"

# --- YouTube-related Settings (REQUIRED for youtube_spider!)
# Set your YouTube API key before trying to run the YouTube-crawler:
YOUTUBE_API_KEY=""
# If you only want to crawl a single YouTube channel/playlist, activate the LIMITED crawl mode by setting its URL here:
# (Please make sure that your 'csv/youtube.csv' contains the same URL!)
#YOUTUBE_LIMITED_CRAWL_URL=""

# --- oeh_spider settings:
# Select the sources you want to fetch from OpenEduHub (comma seperated):
# OEH_IMPORT_SOURCES = 'oeh,wirlernenonline_spider,serlo_spider,youtube_spider'

# Only for spiders based on edu_sharing: Use a saved search (object must be published to everyone in edu-sharing) to query from
# EDU_SHARING_IMPORT_SEARCH_ID = "<node-id>"

# --- SODIX login data (REQUIRED to run sodix_spider!)
# SODIX_SPIDER_USERNAME = "<your_SODIX_API_username>"
# SODIX_SPIDER_PASSWORD = "<your_SODIX_API_password>"
# --- sodix_spider settings:
# SODIX_SPIDER_OER_FILTER=True  # OPTIONAL setting for crawling ONLY OER-compatible materials
# SODIX_SPIDER_START_AT_PAGE="5"  # in case you need to (restart) a crawl process at a specific API page
# (Before using the above setting, please confirm with an API Client like Insomnia if the desired page actually exists!)

# --- serlo_spider (v0.2.8+) settings:
# SERLO_MODIFIED_AFTER="2023-07-01"  # Crawl only Serlo Materials which have been modified (by Serlo authors) after
# <date_of_your_last_crawl>. Use this setting to improve the crawling speed of periodic crawls.
# SERLO_INSTANCE="de"  # optional setting (defaults to: "de" if not actively set)
# Available Serlo "instance" values (as of 2023-08-02): "de" | "en" | "es" | "fr" | "hi" | "ta"

# --- lehreronline_spider Settings
#LO_SKIP_PORTAL="pubertaet;hwm"  # ignore both "Themenportale" at the same time (crawls ONLY Lehrer-Online)
# skip individual "Themenportale" (offerings of cooperation partners) during a crawl by setting one of these values:
# "hwm": skip all URLs from handwerk-macht-schule.de
# "pubertaet": skip all URLs from "Themenportal Pubert√§t"