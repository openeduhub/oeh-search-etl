# Add a url for your log file. If not set, stdoutput will be used
#LOG_FILE = "/var/log/scrapy.log"

# Level for logs, supported DEBUG, INFO, WARNING, ERROR
LOG_LEVEL = "WARNING"

# MODE (edu-sharing, csv, json, or None)
MODE = "csv"

# csv rows to export from dataset (comma seperated, only used if mode == "csv")
CSV_ROWS = "lom.general.title,lom.general.description,lom.general.keyword,lom.technical.location,valuespaces.discipline,valuespaces.learningResourceType"

# Splash Integration settings for the local container,
# for more information, see https://splash.readthedocs.io/en/stable/
DISABLE_SPLASH = False
SPLASH_URL = "http://localhost:8050"

# PYPPETEER Integration settings, as needed for the local container (as used in kmap_spider.py)
# for more information, see: https://github.com/pyppeteer/pyppeteer
PYPPETEER_WS_ENDPOINT="ws://localhost:3000"
# Playwright Integration, as needed for the local container (https://hub.docker.com/r/browserless/chrome#playwright)
PLAYWRIGHT_WS_ENDPOINT="ws://localhost:3000"

# Edu-Sharing instance that the crawlers should upload to
EDU_SHARING_BASE_URL = "http://localhost:8080/edu-sharing/"
EDU_SHARING_USERNAME = "admin"
EDU_SHARING_PASSWORD = "admin"

# Configure if permissions of edu-sharing nodes are handled by the crawler (default true)
# You may want to set this to false if you don't want to apply permissions from crawlers or have a custom implementation in the repository
# EDU_SHARING_PERMISSION_CONTROL=true
# Metadataset to be used for generated nodes. You may use "default" to use the default mds of the repository
# EDU_SHARING_METADATASET=mds_oeh

# If set to true, don't upload to (above mentioned) Edu-Sharing instance
DRY_RUN = True

# you can add one or more custom pipelines here to trigger
# the syntax is: pipeline.package.id:PRIORITY[,pipeline.package.id:PRIORITY,...]
# Use this if you e.g. want to do custom property mapping for any crawler before storing the data
# CUSTOM_PIPELINES = "converter.pipelines.ExampleLoggingPipeline:100"

# your youtube api key (required for youtube crawler)
YOUTUBE_API_KEY = ""

# only for oeh spider: select the sources you want to fetch from oeh (comma seperated)
# OEH_IMPORT_SOURCES = 'oeh,wirlernenonline_spider,serlo_spider,youtube_spider'

# Sodix Spider login data
# SODIX_SPIDER_USERNAME = ""
# SODIX_SPIDER_PASSWORD = ""