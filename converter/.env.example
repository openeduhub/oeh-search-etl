# --- LOGGING-specific settings:
# Add a url for your log file. If not set, stdoutput will be used
#LOG_FILE = "/var/log/scrapy.log"
# Set the level for logs here. Supported values: "DEBUG", "INFO", "WARNING", "ERROR"
LOG_LEVEL = "WARNING"

# --- Crawling-modes: control where crawled items should be stored/exported.
# Available modes: 'edu-sharing', 'csv', 'json' or 'None'
MODE = "csv"
# ------ CSV Export settings (Only used if MODE == "csv"!):
# csv rows to export from dataset (comma seperated! field-names according to items.py!)
CSV_ROWS = "lom.general.title,lom.general.description,lom.general.keyword,lom.technical.location,valuespaces.discipline,valuespaces.learningResourceType"

# --- 'Splash'-Integration settings for the local container,
# for more information, see https://splash.readthedocs.io/en/stable/
DISABLE_SPLASH = False
SPLASH_URL = "http://localhost:8050"

# --- headless-browser settings for the local container:
# PYPPETEER Integration settings, as needed for the local container (as used in kmap_spider.py)
# for more information, see: https://github.com/pyppeteer/pyppeteer
PYPPETEER_WS_ENDPOINT="ws://localhost:3000"
# Playwright Integration, as needed for the local container (https://hub.docker.com/r/browserless/chrome#playwright)
PLAYWRIGHT_WS_ENDPOINT="ws://localhost:3000"

# --- Edu-Sharing instance that the crawlers should upload to
EDU_SHARING_BASE_URL = "http://localhost:8080/edu-sharing/"
EDU_SHARING_USERNAME = "admin"
EDU_SHARING_PASSWORD = "admin"

# Configure if permissions of edu-sharing nodes are handled by the crawler (default true)
# You may want to set this to false if you don't want to apply permissions from crawlers or have a custom implementation in the repository
# EDU_SHARING_PERMISSION_CONTROL=true
# Metadataset to be used for generated nodes. You may use "default" to use the default mds of the repository
# EDU_SHARING_METADATASET=mds_oeh

# If set to true, don't upload to (above mentioned) Edu-Sharing instance
DRY_RUN = True

# --- OERSI-specific settings (oersi_spider):
# Only crawl a specific metadata provider from OERSI (separate multiple providers by semicolon!):
OERSI_METADATA_PROVIDER="KI-Campus;iMoox"
# Continue / complete a previously aborted crawl process by skipping updates of already known items.
# CONTINUE_CRAWL=False
# EDU_SHARING_PRECHECK_SAVED_SEARCH_ID="<insert_your_saved_search_node_id_here>"
# Setting CONTINUE_CRAWL to True will skip all updates of previously crawled items and ONLY crawl new ones!
# ONLY use this mode if you wish to debug/complement/complete huge crawl processes which haven't completed on their own!

# --- Scrapy Pipeline settings:
# You can add one or more custom pipelines here to trigger.
# The syntax is: pipeline.package.id:PRIORITY[,pipeline.package.id:PRIORITY,...]
# Use this if you e.g. want to do custom property mapping for any crawler before storing the data
# CUSTOM_PIPELINES = "converter.pipelines.ExampleLoggingPipeline:100"

# --- YouTube-related Settings (REQUIRED for youtube_spider!)
# Set your YouTube API key before trying to run the YouTube-crawler:
YOUTUBE_API_KEY=""
# If you only want to crawl a single YouTube channel/playlist, activate the LIMITED crawl mode by setting its URL here:
# (Please make sure that your 'csv/youtube.csv' contains the same URL!)
#YOUTUBE_LIMITED_CRAWL_URL=""

# --- oeh_spider settings:
# Select the sources you want to fetch from OpenEduHub (comma seperated):
# OEH_IMPORT_SOURCES = 'oeh,wirlernenonline_spider,serlo_spider,youtube_spider'

# Only for spiders based on edu_sharing: Use a saved search (object must be published to everyone in edu-sharing) to query from
# EDU_SHARING_IMPORT_SEARCH_ID = "<node-id>"

# --- SODIX login data (REQUIRED to run sodix_spider!)
# SODIX_SPIDER_USERNAME = "<your_SODIX_API_username>"
# SODIX_SPIDER_PASSWORD = "<your_SODIX_API_password>"
# --- sodix_spider settings:
# SODIX_SPIDER_OER_FILTER=True  # OPTIONAL setting for crawling ONLY OER-compatible materials

# --- serlo_spider (v0.2.8+) settings:
# SERLO_MODIFIED_AFTER="2023-07-01"  # Crawl only Serlo Materials which have been modified (by Serlo authors) after
# <date_of_your_last_crawl>. Use this setting to improve the crawling speed of periodic crawls.